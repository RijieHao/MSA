import os
import pickle
import cv2
import librosa
import numpy as np
import whisper
from moviepy.video.io.VideoFileClip import VideoFileClip
import mediapipe as mp
from pathlib import Path
import pickle
import h5py
import pandas as pd
from tqdm import tqdm
import gensim.downloader as api
import torch
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler
import warnings
from skimage.feature import hog
import jieba
import fasttext
import fasttext.util

class AccurateMOSEIExtractor:
    """
    æ›´å‡†ç¡®è¿˜åŸMOSEIæ•°æ®é›†ç‰¹å¾æå–çš„ç±»
    åŸºäºè®ºæ–‡æè¿°å®ç°åŸå§‹ç‰¹å¾æå–æ–¹æ³•
    æ”¯æŒä¸­è‹±æ–‡
    """
    
    def __init__(self, language="en"):
        self.language = language
        
        # åŠ è½½Whisperç”¨äºè¯­éŸ³è¯†åˆ«å’Œå¯¹é½
        self.whisper_model = whisper.load_model("base")
        
        # æ ¹æ®è¯­è¨€é€‰æ‹©è¯åµŒå…¥æ¨¡å‹
        if language in ["zh", "chinese", "ä¸­æ–‡"]:
            print("Initializing Chinese word embeddings...")
            self._load_chinese_embeddings()
        else:
            print("Initializing English GloVe embeddings...")
            self._load_english_embeddings()

        # åˆå§‹åŒ–MTCNNæ›¿ä»£å“ (MediaPipe)
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=1, min_detection_confidence=0.5
        )
        
        # åˆå§‹åŒ–é¢éƒ¨å…³é”®ç‚¹æ£€æµ‹ (OpenFaceæ›¿ä»£)
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        print(f"Initialized AccurateMOSEIExtractor for language: {language}")
    
    def _load_chinese_embeddings(self):
        """åŠ è½½ä¸­æ–‡è¯åµŒå…¥æ¨¡å‹ï¼ˆå¯¹æ ‡GloVeï¼‰"""
        try:
            # æ–¹æ¡ˆ1: å°è¯•åŠ è½½FastTextä¸­æ–‡æ¨¡å‹
            self._load_fasttext_chinese()
        except Exception as e:
            print(f"Failed to load FastText: {e}")
            try:
                # æ–¹æ¡ˆ2: å°è¯•åŠ è½½é¢„è®­ç»ƒä¸­æ–‡Word2Vec
                self._load_chinese_word2vec()
            except Exception as e2:
                print(f"Failed to load Word2Vec: {e2}")

    def _load_fasttext_chinese(self):
        """ä½¿ç”¨FastTextä¸­æ–‡æ¨¡å‹"""
        try:
            print("Loading FastText Chinese model...")
            # ä¸‹è½½ä¸­æ–‡FastTextæ¨¡å‹
            fasttext.util.download_model('zh', if_exists='ignore')
            self.word_model = fasttext.load_model('cc.zh.300.bin')
            self.word_embedding_dim = 300
            self.embedding_type = "fasttext"
            print("âœ“ Loaded FastText Chinese: 300d")
            
        except Exception as e:
            raise e

    def _load_chinese_word2vec(self):
        """åŠ è½½é¢„è®­ç»ƒä¸­æ–‡Word2Vecæ¨¡å‹"""
        try:
            from gensim.models import KeyedVectors
            
            # å¯èƒ½çš„ä¸­æ–‡Word2Vecæ¨¡å‹è·¯å¾„
            model_paths = [
                "data/embeddings/chinese_word2vec_300d.txt",
                "data/embeddings/sgns.weibo.bigram-char",
                "data/embeddings/tencent-ailab-embedding-zh-d200-v0.2.0.txt"
            ]
            
            for model_path in model_paths:
                if Path(model_path).exists():
                    print(f"Loading Chinese Word2Vec from {model_path}...")
                    self.word_model = KeyedVectors.load_word2vec_format(
                        model_path, binary=False, unicode_errors='ignore'
                    )
                    self.word_embedding_dim = self.word_model.vector_size
                    self.embedding_type = "word2vec"
                    print(f"âœ“ Loaded Chinese Word2Vec: {self.word_embedding_dim}d")
                    return
            
            raise FileNotFoundError("No Chinese Word2Vec model found")
            
        except Exception as e:
            raise e

    def _load_english_embeddings(self):
        """åŠ è½½è‹±æ–‡GloVeåµŒå…¥"""
        try:
            import gensim.downloader as api
            print("Loading GloVe word embeddings...")
            self.word_model = api.load("glove-wiki-gigaword-300")
            self.word_embedding_dim = 300
            self.embedding_type = "glove"
            print("âœ“ Loaded GloVe: 300d")
        except Exception as e:
            print(f"Failed to load GloVe: {e}")
            self.word_model = None
            self.word_embedding_dim = 300
            self.embedding_type = "random"

    def extract_word_features(self, video_path):
        """
        æå–åŸºäºè¯åµŒå…¥çš„ç‰¹å¾ - ä¸­è‹±æ–‡ç»Ÿä¸€æ¥å£
        
        Returns:
            list: [[word_embedding, start_time, end_time], ...] æ ¼å¼
        """
        # 1. ä»è§†é¢‘æå–éŸ³é¢‘
        video = VideoFileClip(video_path)
        temp_audio = f"temp_audio_for_{self.language}.wav"
        video.audio.write_audiofile(temp_audio, logger=None)
        
        # 2. ä½¿ç”¨Whisperè¿›è¡Œè¯çº§å¯¹é½
        whisper_language = "zh" if self.language in ["zh", "chinese", "ä¸­æ–‡"] else self.language
        result = self.whisper_model.transcribe(
            temp_audio, 
            language=whisper_language,
            word_timestamps=True
        )
        
        # 3. è½¬æ¢ä¸ºè¯åµŒå…¥åºåˆ—
        word_features = []
        
        if 'segments' in result:
            for segment in result['segments']:
                if 'words' in segment:
                    for word_info in segment['words']:
                        word = word_info.get('word', '').strip()
                        start = word_info.get('start', 0.0)
                        end = word_info.get('end', 0.0)
                        
                        if word:
                            if self.language in ["zh", "chinese", "ä¸­æ–‡"]:
                                # ä¸­æ–‡åˆ†è¯å¤„ç†
                                segmented_words = self._chinese_word_segmentation(word)
                                for seg_word in segmented_words:
                                    if seg_word.strip():
                                        word_embedding = self._get_chinese_word_embedding(seg_word)
                                        word_features.append([word_embedding, start, end])
                            else:
                                # è‹±æ–‡å¤„ç†
                                word_clean = word.lower().strip()
                                word_embedding = self._get_english_word_embedding(word_clean)
                                word_features.append([word_embedding, start, end])
        
        # 4. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_audio)
        video.close()
        
        return word_features

    def _chinese_word_segmentation(self, text):
        """ä¸­æ–‡åˆ†è¯"""
        try:
            words = list(jieba.cut(text, cut_all=False))
            return [w.strip() for w in words if w.strip()]
        except ImportError:
            print("Warning: jieba not available, using character-level segmentation")
            # å¦‚æœjiebaä¸å¯ç”¨ï¼ŒæŒ‰å­—ç¬¦åˆ†å‰²
            return list(text)

    def _get_chinese_word_embedding(self, word):
        """è·å–ä¸­æ–‡è¯åµŒå…¥"""
        if self.embedding_type == "fasttext":
            return self.word_model.get_word_vector(word).astype(np.float32)
        elif self.embedding_type == "word2vec":
            if word in self.word_model:
                return self.word_model[word].astype(np.float32)
            else:
                return np.zeros(self.word_embedding_dim, dtype=np.float32)
        elif self.embedding_type == "char":
            return self._get_character_embedding(word)
        else:
            return np.zeros(self.word_embedding_dim, dtype=np.float32)

    def _get_character_embedding(self, word):
        """è·å–è¯çš„å­—ç¬¦çº§å¹³å‡åµŒå…¥"""
        char_embeddings = []
        
        for char in word:
            if char in self.char_vocab:
                char_idx = self.char_vocab[char]
            else:
                char_idx = self.char_vocab['<UNK>']
            
            char_embeddings.append(self.char_embeddings[char_idx])
        
        if char_embeddings:
            # è¿”å›å­—ç¬¦åµŒå…¥çš„å¹³å‡å€¼
            return np.mean(char_embeddings, axis=0).astype(np.float32)
        else:
            return np.zeros(self.word_embedding_dim, dtype=np.float32)

    def _get_english_word_embedding(self, word):
        """è·å–è‹±æ–‡è¯åµŒå…¥ï¼ˆGloVeï¼‰"""
        if self.embedding_type == "glove" and self.word_model and word in self.word_model:
            return self.word_model[word].astype(np.float32)
        else:
            # å¦‚æœè¯æ±‡ä¸åœ¨GloVeä¸­ï¼Œè¿”å›é›¶å‘é‡
            return np.zeros(self.word_embedding_dim, dtype=np.float32)

    # ä¿æŒåŸæœ‰çš„éŸ³é¢‘å’Œè§†è§‰ç‰¹å¾æå–æ–¹æ³•ä¸å˜
    def extract_covarep_acoustic_features(self, video_path):
        """
        æå–COVAREPé£æ ¼çš„å£°å­¦ç‰¹å¾ - å®Œå…¨ä¿®å¤ç‰ˆæœ¬
        
        Returns:
            np.ndarray: shape [æ—¶é—´æ­¥æ•°,40] çš„COVAREPå…¼å®¹ç‰¹å¾
        """
        try:
            print(f"ğŸ”Š æå–éŸ³é¢‘ç‰¹å¾: {video_path}")
            
            # 1. ä»è§†é¢‘æå–éŸ³é¢‘
            video = VideoFileClip(video_path)
            frame_rate = video.fps  # è·å–å®é™…å¸§ç‡
            if video.audio is None:
                print(f"âš ï¸ è§†é¢‘æ²¡æœ‰éŸ³é¢‘è½¨é“")
                video.close()
                return np.zeros((1, 40))
            
            temp_audio = "temp_audio_for_covarep.wav"
            
            # å¼ºåˆ¶ä½¿ç”¨ 16kHz é¿å…é‡‡æ ·ç‡é—®é¢˜
            video.audio.write_audiofile(temp_audio, 
                                       fps=22050,  # å¼ºåˆ¶16kHz
                                       logger=None,
                                       )
            video.close()
            
            # 2. ä½¿ç”¨librosaåŠ è½½éŸ³é¢‘ï¼Œç¡®ä¿é‡‡æ ·ç‡
            y, sr = librosa.load(temp_audio, sr=22050)  # å¼ºåˆ¶16kHz
            
            print(f"ğŸµ éŸ³é¢‘ä¿¡æ¯: {len(y)/sr:.2f}ç§’, é‡‡æ ·ç‡: {sr}Hz")
            
            # 3. è®¡ç®—å¸§æ•° (ä¸è§†é¢‘å¯¹é½)
            video_duration = len(y) / sr
            n_frames = max(1, int(video_duration * frame_rate))
            
            # 4. æå–ç‰¹å¾
            features_sequence = []
            hop_length = max(1, len(y) // n_frames)
            
            for i in range(n_frames):
                start_idx = i * hop_length
                end_idx = min((i + 1) * hop_length, len(y))
                
                if start_idx >= len(y):
                    break
                    
                y_segment = y[start_idx:end_idx]
                
                if len(y_segment) == 0:
                    frame_features = np.zeros(40)
                else:
                    frame_features = self._extract_covarep_frame_features(y_segment, sr)
                
                features_sequence.append(frame_features)
            
            # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_audio):
                os.remove(temp_audio)
            
            acoustic_features = np.array(features_sequence)
            print(f"âœ… å£°å­¦ç‰¹å¾å½¢çŠ¶: {acoustic_features.shape}")
            return acoustic_features
            
        except Exception as e:
            print(f"âŒ å£°å­¦ç‰¹å¾æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_audio = "temp_audio_for_covarep.wav"
            if os.path.exists(temp_audio):
                os.remove(temp_audio)
            
            return np.zeros((1, 40))
    
    def _extract_covarep_frame_features(self, y_segment, sr):
        """æŒ‰COVAREPæ ‡å‡†æå–å•å¸§40ç»´ç‰¹å¾ - ä¿®å¤ç‰ˆæœ¬"""
        features = []
        fmin = 85
        frame_length = min(369, len(y_segment) // 2)   
        try:
            # 1. 12ä¸ªMFCCç³»æ•°
            if len(y_segment) > 512:
                mfcc = librosa.feature.mfcc(y=y_segment, sr=sr, n_mfcc=12)
                mfcc_mean = np.mean(mfcc, axis=1) if mfcc.shape[1] > 0 else np.zeros(12)
                features.extend(mfcc_mean)
            else:
                features.extend([0.0] * 12)
            
            # 2. åŸºé¢‘ç‰¹å¾ (Pitch) - 8ç»´ - å®Œå…¨ä¿®å¤
            try:
                if len(y_segment) > frame_length and frame_length > 0:
                    f0 = librosa.yin(y_segment, 
                                   fmin=fmin, 
                                   fmax=min(400, sr//4),
                                   frame_length=frame_length)
                    
                    if len(f0) > 0:
                        f0_clean = f0[f0 > 0]
                        if len(f0_clean) > 0:
                            pitch_features = [
                                np.mean(f0_clean),
                                np.std(f0_clean),
                                np.median(f0_clean),
                                np.percentile(f0_clean, 25),
                                np.percentile(f0_clean, 75),
                                np.min(f0_clean),
                                np.max(f0_clean),
                                len(f0_clean) / len(f0)
                            ]
                        else:
                            pitch_features = [0.0] * 8
                    else:
                        pitch_features = [0.0] * 8
                else:
                    pitch_features = [0.0] * 8
                
            except Exception as pitch_error:
                print(f"âš ï¸ Pitch extraction failed: {pitch_error}")
                pitch_features = [0.0] * 8
                
            features.extend(pitch_features)
            
            # 3. æµŠéŸ³/æ¸…éŸ³åˆ†å‰²ç‰¹å¾ (Voiced/Unvoiced) - 6ç»´ - ä¿®å¤APIè°ƒç”¨
            if len(y_segment) > 512:
                try:
                    # é›¶äº¤å‰ç‡
                    zcr = np.mean(librosa.feature.zero_crossing_rate(y_segment))
                    
                    # é¢‘è°±é‡å¿ƒ
                    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y_segment, sr=sr))
                    
                    # é¢‘è°±å¸¦å®½
                    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y_segment, sr=sr))
                    
                    # é¢‘è°±å¹³å¦åº¦ - ä¿®å¤APIè°ƒç”¨
                    spectral_flatness = np.mean(librosa.feature.spectral_flatness(y=y_segment))
                    
                    # èƒ½é‡
                    energy = np.sum(y_segment ** 2) / len(y_segment)
                    
                    # è°æ³¢å™ªå£°æ¯”
                    try:
                        harmonic, percussive = librosa.effects.hpss(y_segment)
                        hnr = np.sum(harmonic ** 2) / (np.sum(percussive ** 2) + 1e-8)
                    except:
                        hnr = 1.0  # é»˜è®¤å€¼
                    
                    voiced_unvoiced_features = [zcr, spectral_centroid, spectral_bandwidth, 
                                              spectral_flatness, energy, hnr]
                except Exception as vu_error:
                    print(f"âš ï¸ Voiced/Unvoiced features failed: {vu_error}")
                    voiced_unvoiced_features = [0.0] * 6
            else:
                voiced_unvoiced_features = [0.0] * 6
            features.extend(voiced_unvoiced_features)
        
            
            # 5. å³°å€¼æ–œç‡å‚æ•° (Peak Slope Parameters) - 4ç»´
            if len(y_segment) > 256:
                try:
                    hop_length = min(512, len(y_segment)//4)
                    stft = librosa.stft(y_segment, hop_length=hop_length)
                    envelope = np.abs(stft)
                    envelope_mean = np.mean(envelope, axis=0)
                    
                    if len(envelope_mean) > 2:
                        envelope_diff = np.diff(envelope_mean)
                        slope_features = [
                            np.mean(envelope_diff[envelope_diff > 0]) if np.any(envelope_diff > 0) else 0.0,
                            np.mean(envelope_diff[envelope_diff < 0]) if np.any(envelope_diff < 0) else 0.0,
                            np.std(envelope_diff),
                            np.max(np.abs(envelope_diff))
                        ]
                        slope_features = [x if not np.isnan(x) else 0.0 for x in slope_features]
                    else:
                        slope_features = [0.0] * 4
                except Exception as slope_error:
                    print(f"âš ï¸ Slope features failed: {slope_error}")
                    slope_features = [0.0] * 4
            else:
                slope_features = [0.0] * 4
            features.extend(slope_features)
            
            # 6. æœ€å¤§åˆ†æ•£å•† (Maxima Dispersion Quotients) - 4ç»´
            if len(y_segment) > 512:
                try:
                    from scipy.signal import find_peaks
                    peaks, _ = find_peaks(np.abs(y_segment), height=0.01)
                    
                    if len(peaks) > 1:
                        peak_intervals = np.diff(peaks)
                        dispersion_features = [
                            np.mean(peak_intervals),
                            np.std(peak_intervals),
                            np.min(peak_intervals),
                            np.max(peak_intervals)
                        ]
                    else:
                        dispersion_features = [0.0] * 4
                except Exception as disp_error:
                    print(f"âš ï¸ Dispersion features failed: {disp_error}")
                    dispersion_features = [0.0] * 4
            else:
                dispersion_features = [0.0] * 4
            features.extend(dispersion_features)
            
            # 7. å…¶ä»–æƒ…æ„Ÿç›¸å…³ç‰¹å¾è¡¥é½40ç»´
            remaining_dims = 40 - len(features)
            if remaining_dims > 0:
                try:
                    if len(y_segment) > 512:
                        hop_length = min(512, len(y_segment)//4)
                        chroma = librosa.feature.chroma_stft(y=y_segment, sr=sr, hop_length=hop_length)
                        chroma_mean = np.mean(chroma, axis=1) if chroma.shape[1] > 0 else np.zeros(12)
                        additional_features = list(chroma_mean[:remaining_dims])
                    else:
                        additional_features = [0.0] * remaining_dims
                except Exception as chroma_error:
                    print(f"âš ï¸ Chroma features failed: {chroma_error}")
                    additional_features = [0.0] * remaining_dims
                features.extend(additional_features)
            
            # ç¡®ä¿æ­£å¥½40ç»´
            features = features[:40]
            while len(features) < 40:
                features.append(0.0)
            
        except Exception as e:
            print(f"âŒ Error extracting COVAREP features: {e}")
            features = [0.0] * 40
        
        return np.array(features, dtype=np.float32)
    
    def extract_openface_visual_features(self, video_path):
        """
        æå–OpenFaceé£æ ¼çš„è§†è§‰ç‰¹å¾ - è¿˜åŸåŸå§‹MOSEIè§†è§‰ç‰¹å¾
        åŒ…å«68ä¸ªé¢éƒ¨å…³é”®ç‚¹ã€20ä¸ªå½¢çŠ¶å‚æ•°ã€HoGç‰¹å¾ã€å¤´éƒ¨å§¿æ€ã€çœ¼éƒ¨æ³¨è§†
        
        Returns:
            np.ndarray: shape [æ—¶é—´æ­¥æ•°, 709] çš„OpenFaceå…¼å®¹ç‰¹å¾
        """
        cap = cv2.VideoCapture(video_path)
        features_sequence = []
        
        frame_count = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame_count += 1
            
            # MTCNNæ›¿ä»£: ä½¿ç”¨MediaPipeè¿›è¡Œäººè„¸æ£€æµ‹
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            detection_results = self.face_detection.process(rgb_frame)
            
            # é¢éƒ¨å…³é”®ç‚¹æ£€æµ‹
            mesh_results = self.face_mesh.process(rgb_frame)
            
            if mesh_results.multi_face_landmarks and detection_results.detections:
                landmarks = mesh_results.multi_face_landmarks[0]
                detection = detection_results.detections[0]
                frame_features = self._extract_openface_frame_features(
                    landmarks, detection, frame
                )
            else:
                # æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸æ—¶ä½¿ç”¨é›¶ç‰¹å¾
                frame_features = np.zeros(35)
            
            features_sequence.append(frame_features)
        
        cap.release()
        
        if len(features_sequence) == 0:
            features_sequence = [np.zeros(35)]
        
        return np.array(features_sequence)
    

    '''
    ä¸‹é¢æ˜¯709ç»´è„¸éƒ¨ç‰¹å¾æå–çš„å†…å®¹çš„ç®€åŒ–ï¼Œæ³¨é‡Šå‰ä¸º709ç»´
    æ³¨é‡Šåä¸º35ç»´
    '''
    def _extract_openface_frame_features(self, landmarks, detection, frame):
        """æŒ‰OpenFaceæ ‡å‡†æå–å•å¸§709ç»´ç‰¹å¾"""
        features = []
        h, w = frame.shape[:2]
        
        # 1. 68ä¸ªé¢éƒ¨å…³é”®ç‚¹åæ ‡ (136ç»´: 68ç‚¹ Ã— 2åæ ‡)
        #landmark_coords = self._extract_68_facial_landmarks(landmarks)
        #features.extend(landmark_coords)  # 136ç»´

        #1.10ä¸ªå…³é”®ç‚¹åæ ‡ï¼ˆé€‰å–éƒ¨åˆ†æœ‰ä»£è¡¨æ€§çš„ç‚¹ï¼‰(20ç»´)
        key_indices = [1, 33, 263, 61, 291, 199, 234, 454, 10, 152]  # é¼»å°–ã€å·¦å³çœ¼ã€å˜´è§’ã€è„¸é¢Šç­‰
        for idx in key_indices:
            if idx < len(landmarks.landmark):
                landmark = landmarks.landmark[idx]
                features.extend([landmark.x, landmark.y])
            else:
                features.extend([0.0, 0.0])
        
        # 2. 20ä¸ªé¢éƒ¨å½¢çŠ¶å‚æ•° (PCAé™ç»´åçš„å½¢çŠ¶æè¿°)
        #shape_params = self._extract_facial_shape_parameters(landmarks)
        #features.extend(shape_params)  # 20ç»´
        
        # 3. é¢éƒ¨HoGç‰¹å¾ (ç®€åŒ–ç‰ˆæœ¬3ç»´)
        hog_features = self._extract_facial_hog_features(landmarks, frame)
        #features.extend(hog_features)  # 100ç»´
        features.extend(hog_features[:3])  # 3ç»´
        
        # 4. å¤´éƒ¨å§¿æ€ (6ç»´: roll, pitch, yaw, x, y, z)(ç®€åŒ–3ç»´)
        head_pose = self._extract_head_pose(landmarks, frame.shape)
        #features.extend(head_pose)  # 6ç»´
        features.extend(head_pose[:3])  # 3ç»´
        
        # 5. çœ¼éƒ¨æ³¨è§†æ–¹å‘ (4ç»´)
        eye_gaze = self._extract_eye_gaze(landmarks)
        features.extend(eye_gaze)  # 4ç»´
        
        # 6. FACSåŠ¨ä½œå•å…ƒ (17ç»´AUå¼ºåº¦)(ç®€åŒ–5ç»´)
        action_units = self._extract_facial_action_units(landmarks)
        #features.extend(action_units)  # 17ç»´
        features.extend(action_units[:5])  # 5ç»´
        
        # 7. 6ç§åŸºæœ¬æƒ…æ„Ÿ (Emotient FACETé£æ ¼)
        #basic_emotions = self._extract_basic_emotions(landmarks)
        #features.extend(basic_emotions)  # 6ç»´
        
        # 8. æ·±åº¦äººè„¸åµŒå…¥ç‰¹å¾ (ç®€åŒ–ç‰ˆ)
        #face_embeddings = self._extract_face_embeddings(landmarks, frame)
        #features.extend(face_embeddings)  # 128ç»´
        
        # 9. å…¶ä»–ç‰¹å¾è¡¥é½åˆ°709ç»´
        #remaining_dims = 709 - len(features)
        #if remaining_dims > 0:
        #    additional_features = self._extract_additional_visual_features(
        #        landmarks, frame, remaining_dims
        #    )
        #    features.extend(additional_features)
        
        # ç¡®ä¿æ­£å¥½709ç»´
        #features = features[:709]
        features = features[:35]
        #while len(features) < 709:
        while len(features) < 35:
            features.append(0.0)
        
        return np.array(features, dtype=np.float32)
    
    def _extract_68_facial_landmarks(self, landmarks):
        """æå–68ä¸ªé¢éƒ¨å…³é”®ç‚¹ (OpenFaceæ ‡å‡†)"""
        # MediaPipeæœ‰468ä¸ªç‚¹ï¼Œé€‰æ‹©å¯¹åº”OpenFaceçš„68ä¸ªå…³é”®ç‚¹
        openface_68_indices = [
            # ä¸‹å·´è½®å»“ (17ä¸ªç‚¹: 0-16)
            172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379, 365, 397, 288, 361, 323, 454,
            # å³çœ‰æ¯› (5ä¸ªç‚¹: 17-21)
            70, 63, 105, 66, 107,
            # å·¦çœ‰æ¯› (5ä¸ªç‚¹: 22-26)
            55, 65, 52, 53, 46,
            # é¼»å­ (9ä¸ªç‚¹: 27-35)
            168, 8, 9, 10, 151, 195, 197, 196, 3,
            # å³çœ¼ (6ä¸ªç‚¹: 36-41)
            33, 7, 163, 144, 145, 153,
            # å·¦çœ¼ (6ä¸ªç‚¹: 42-47)
            362, 382, 381, 380, 374, 373,
            # å˜´éƒ¨å¤–è½®å»“ (12ä¸ªç‚¹: 48-59)
            61, 84, 17, 314, 405, 320, 307, 375, 321, 308, 324, 318,
            # å˜´éƒ¨å†…è½®å»“ (8ä¸ªç‚¹: 60-67)
            78, 95, 88, 178, 87, 14, 317, 402
        ]
        
        coords = []
        for i in range(68):
            if i < len(openface_68_indices):
                idx = openface_68_indices[i]
                if idx < len(landmarks.landmark):
                    landmark = landmarks.landmark[idx]
                    coords.extend([landmark.x, landmark.y])
                else:
                    coords.extend([0.0, 0.0])
            else:
                coords.extend([0.0, 0.0])
        
        return coords[:136]  # ç¡®ä¿136ç»´
    
    def _extract_facial_shape_parameters(self, landmarks):
        """æå–20ä¸ªé¢éƒ¨å½¢çŠ¶å‚æ•° (PCAé™ç»´)"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºå…³é”®ç‚¹è®¡ç®—å½¢çŠ¶æè¿°ç¬¦
        params = []
        
        # é¢éƒ¨å®½é«˜æ¯”
        face_width = abs(landmarks.landmark[234].x - landmarks.landmark[454].x)
        face_height = abs(landmarks.landmark[10].y - landmarks.landmark[152].y)
        params.append(face_width / (face_height + 1e-8))
        
        # çœ¼ç›å®½åº¦æ¯”ä¾‹
        left_eye_width = abs(landmarks.landmark[33].x - landmarks.landmark[133].x)
        right_eye_width = abs(landmarks.landmark[362].x - landmarks.landmark[263].x)
        params.extend([left_eye_width, right_eye_width])
        
        # å˜´å·´å‚æ•°
        mouth_width = abs(landmarks.landmark[61].x - landmarks.landmark[291].x)
        mouth_height = abs(landmarks.landmark[13].y - landmarks.landmark[14].y)
        params.extend([mouth_width, mouth_height])
        
        # é¼»å­å‚æ•°
        nose_width = abs(landmarks.landmark[125].x - landmarks.landmark[141].x)
        nose_height = abs(landmarks.landmark[19].y - landmarks.landmark[1].y)
        params.extend([nose_width, nose_height])
        
        # è¡¥é½åˆ°20ç»´
        while len(params) < 20:
            params.append(0.0)
        
        return params[:20]
    
    def _extract_facial_hog_features(self, landmarks, frame):
        """æå–é¢éƒ¨HoGç‰¹å¾ (ç®€åŒ–ç‰ˆæœ¬)"""
        try:
            
            # æå–é¢éƒ¨åŒºåŸŸ
            face_region = self._extract_face_region(landmarks, frame)
            
            if face_region is not None:
                # è½¬æ¢ä¸ºç°åº¦
                gray_face = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)
                
                # è°ƒæ•´å¤§å°
                gray_face = cv2.resize(gray_face, (64, 64))
                
                # æå–HoGç‰¹å¾
                hog_features = hog(
                    gray_face,
                    orientations=9,
                    pixels_per_cell=(8, 8),
                    cells_per_block=(2, 2),
                    block_norm='L2-Hys',
                    feature_vector=True
                )
                
                # é™ç»´åˆ°100ç»´
                if len(hog_features) > 100:
                    hog_features = hog_features[:100]
                else:
                    hog_features = list(hog_features) + [0.0] * (100 - len(hog_features))
            else:
                hog_features = [0.0] * 100
                
        except Exception as e:
            hog_features = [0.0] * 100
        
        return hog_features
    
    def _extract_face_region(self, landmarks, frame):
        """ä»frameä¸­æå–é¢éƒ¨åŒºåŸŸ"""
        try:
            h, w = frame.shape[:2]
            
            # è·å–é¢éƒ¨è¾¹ç•Œæ¡†
            x_coords = [landmarks.landmark[i].x * w for i in range(len(landmarks.landmark))]
            y_coords = [landmarks.landmark[i].y * h for i in range(len(landmarks.landmark))]
            
            x1, x2 = int(min(x_coords)), int(max(x_coords))
            y1, y2 = int(min(y_coords)), int(max(y_coords))
            
            # æ·»åŠ è¾¹è·
            margin = 10
            x1 = max(0, x1 - margin)
            y1 = max(0, y1 - margin)
            x2 = min(w, x2 + margin)
            y2 = min(h, y2 + margin)
            
            face_region = frame[y1:y2, x1:x2]
            
            if face_region.size > 0:
                return face_region
            else:
                return None
                
        except Exception as e:
            return None
    
    def _extract_head_pose(self, landmarks, frame_shape):
        """æå–å¤´éƒ¨å§¿æ€ (6ç»´)"""
        # ç®€åŒ–çš„å¤´éƒ¨å§¿æ€ä¼°è®¡
        nose_tip = landmarks.landmark[1]
        left_eye = landmarks.landmark[33]
        right_eye = landmarks.landmark[362]
        
        # Roll (å¤´éƒ¨å€¾æ–œ)
        eye_angle = np.arctan2(right_eye.y - left_eye.y, right_eye.x - left_eye.x)
        roll = np.degrees(eye_angle)
        
        # Pitch (ä¿¯ä»°)
        pitch = (nose_tip.y - 0.5) * 60
        
        # Yaw (åèˆª)
        face_center_x = (left_eye.x + right_eye.x) / 2
        yaw = (nose_tip.x - face_center_x) * 120
        
        # ä½ç½® (ç›¸å¯¹äºå›¾åƒä¸­å¿ƒ)
        h, w = frame_shape[:2]
        x_pos = (nose_tip.x - 0.5) * w
        y_pos = (nose_tip.y - 0.5) * h
        z_pos = abs(landmarks.landmark[10].y - landmarks.landmark[152].y) * 100
        
        return [roll, pitch, yaw, x_pos, y_pos, z_pos]
    
    def _extract_eye_gaze(self, landmarks):
        """æå–çœ¼éƒ¨æ³¨è§†æ–¹å‘ (4ç»´)"""
        # ç®€åŒ–çš„æ³¨è§†æ–¹å‘ä¼°è®¡
        left_eye_center = np.mean([[landmarks.landmark[i].x, landmarks.landmark[i].y] 
                                  for i in [33, 133]], axis=0)
        right_eye_center = np.mean([[landmarks.landmark[i].x, landmarks.landmark[i].y] 
                                   for i in [362, 263]], axis=0)
        
        # è®¡ç®—æ³¨è§†æ–¹å‘
        left_gaze_x = (left_eye_center[0] - 0.3) * 2
        left_gaze_y = (left_eye_center[1] - 0.4) * 2
        right_gaze_x = (right_eye_center[0] - 0.7) * 2
        right_gaze_y = (right_eye_center[1] - 0.4) * 2
        
        return [left_gaze_x, left_gaze_y, right_gaze_x, right_gaze_y]
    
    def _extract_facial_action_units(self, landmarks):
        """æå–FACSåŠ¨ä½œå•å…ƒå¼ºåº¦ (17ç»´)"""
        # åŸºäºé¢éƒ¨å…³é”®ç‚¹ä¼°è®¡ä¸»è¦AU
        aus = []
        
        # AU1 - å†…çœ‰ä¸ŠæŠ¬
        au1 = max(0, 0.5 - landmarks.landmark[55].y) * 10
        aus.append(au1)
        
        # AU2 - å¤–çœ‰ä¸ŠæŠ¬
        au2 = max(0, 0.4 - landmarks.landmark[70].y) * 10
        aus.append(au2)
        
        # AU4 - çœ‰å¤´ç´§é”
        brow_distance = abs(landmarks.landmark[55].x - landmarks.landmark[70].x)
        au4 = max(0, 0.1 - brow_distance) * 50
        aus.append(au4)
        
        # AU5 - ä¸Šçœ¼ç‘ä¸ŠæŠ¬
        left_eye_open = abs(landmarks.landmark[33].y - landmarks.landmark[145].y)
        au5 = left_eye_open * 20
        aus.append(au5)
        
        # AU6 - è„¸é¢Šä¸ŠæŠ¬
        cheek_height = landmarks.landmark[116].y
        au6 = max(0, 0.6 - cheek_height) * 15
        aus.append(au6)
        
        # AU9 - é¼»çš±
        nose_width = abs(landmarks.landmark[125].x - landmarks.landmark[141].x)
        au9 = max(0, nose_width - 0.02) * 100
        aus.append(au9)
        
        # AU12 - å˜´è§’ä¸Šæ‰¬
        mouth_corner_avg = (landmarks.landmark[61].y + landmarks.landmark[84].y) / 2
        mouth_center = landmarks.landmark[13].y
        au12 = max(0, mouth_center - mouth_corner_avg) * 50
        aus.append(au12)
        
        # AU15 - å˜´è§’ä¸‹æ‹‰
        au15 = max(0, mouth_corner_avg - mouth_center) * 50
        aus.append(au15)
        
        # AU20 - å˜´å”‡æ°´å¹³æ‹‰ä¼¸
        mouth_width = abs(landmarks.landmark[61].x - landmarks.landmark[291].x)
        au20 = mouth_width * 100
        aus.append(au20)
        
        # AU25 - å˜´å”‡åˆ†ç¦»
        mouth_open = abs(landmarks.landmark[13].y - landmarks.landmark[14].y)
        au25 = mouth_open * 100
        aus.append(au25)
        
        # è¡¥é½åˆ°17ç»´
        while len(aus) < 17:
            aus.append(0.0)
        
        return aus[:17]
    
    def _extract_basic_emotions(self, landmarks):
        """æå–6ç§åŸºæœ¬æƒ…æ„Ÿ (Emotient FACETé£æ ¼)"""
        # åŸºäºé¢éƒ¨å…³é”®ç‚¹çš„ç®€åŒ–æƒ…æ„Ÿè¯†åˆ«
        emotions = []
        
        # Happiness - åŸºäºå˜´è§’ä¸Šæ‰¬
        mouth_corner_lift = self._calculate_mouth_corner_lift(landmarks)
        happiness = max(0, mouth_corner_lift) * 5
        emotions.append(happiness)
        
        # Sadness - åŸºäºå˜´è§’ä¸‹æ‹‰å’Œçœ‰æ¯›ä¸‹å‚
        mouth_corner_drop = -min(0, mouth_corner_lift)
        brow_drop = max(0, 0.45 - landmarks.landmark[55].y)
        sadness = (mouth_corner_drop + brow_drop) * 3
        emotions.append(sadness)
        
        # Anger - åŸºäºçœ‰æ¯›ç´§é”
        brow_furrow = max(0, 0.1 - abs(landmarks.landmark[55].x - landmarks.landmark[70].x))
        anger = brow_furrow * 20
        emotions.append(anger)
        
        # Disgust - åŸºäºé¼»å­çš±èµ·
        nose_scrunch = abs(landmarks.landmark[125].x - landmarks.landmark[141].x)
        disgust = max(0, nose_scrunch - 0.02) * 50
        emotions.append(disgust)
        
        # Surprise - åŸºäºçœ‰æ¯›ä¸ŠæŠ¬å’Œå˜´å·´å¼ å¼€
        brow_raise = max(0, 0.4 - landmarks.landmark[70].y)
        mouth_open = abs(landmarks.landmark[13].y - landmarks.landmark[14].y)
        surprise = (brow_raise + mouth_open) * 10
        emotions.append(surprise)
        
        # Fear - åŸºäºçœ¼ç›å¼ å¤§å’Œçœ‰æ¯›ä¸ŠæŠ¬
        eye_wide = abs(landmarks.landmark[33].y - landmarks.landmark[145].y)
        fear = (eye_wide + brow_raise) * 8
        emotions.append(fear)
        
        return emotions
    
    def _calculate_mouth_corner_lift(self, landmarks):
        """è®¡ç®—å˜´è§’ä¸Šæ‰¬ç¨‹åº¦"""
        left_corner = landmarks.landmark[61].y
        right_corner = landmarks.landmark[84].y
        mouth_center = landmarks.landmark[13].y
        
        corner_avg = (left_corner + right_corner) / 2
        return mouth_center - corner_avg
    
    def _extract_face_embeddings(self, landmarks, frame):
        """æå–æ·±åº¦äººè„¸åµŒå…¥ (ç®€åŒ–ç‰ˆæœ¬)"""
        # è¿™é‡Œå¯ä»¥é›†æˆçœŸæ­£çš„äººè„¸è¯†åˆ«æ¨¡å‹å¦‚FaceNet
        # ç›®å‰ä½¿ç”¨åŸºäºå…³é”®ç‚¹çš„ç®€åŒ–åµŒå…¥
        embedding = []
        
        # åŸºäºå…³é”®ç‚¹è®¡ç®—å‡ ä½•ç‰¹å¾ä½œä¸ºåµŒå…¥
        for i in range(0, min(128, len(landmarks.landmark))):
            if i < len(landmarks.landmark):
                landmark = landmarks.landmark[i]
                embedding.extend([landmark.x, landmark.y])
            else:
                embedding.extend([0.0, 0.0])
        
        # è°ƒæ•´åˆ°128ç»´
        while len(embedding) < 128:
            embedding.append(0.0)
        
        return embedding[:128]
    
    def _extract_additional_visual_features(self, landmarks, frame, num_features):
        """æå–é¢å¤–çš„è§†è§‰ç‰¹å¾ä»¥è¾¾åˆ°709ç»´"""
        features = []
        
        # æ·»åŠ æ›´å¤šå‡ ä½•ç‰¹å¾
        for i in range(num_features):
            if i < len(landmarks.landmark):
                landmark = landmarks.landmark[i % len(landmarks.landmark)]
                features.append(landmark.x * landmark.y)  # ç®€å•çš„ç»„åˆç‰¹å¾
            else:
                features.append(0.0)
        
        return features[:num_features]
    
    def save_to_csd_format(self, data, output_path, description="", metadata={}):
        with h5py.File(output_path, 'w') as f:
            # åˆ›å»ºé¡¶çº§ç»„ computational_sequences
            top_group = f.create_group("computational_sequences")
            
            # åˆ›å»º data ç»„
            data_group = top_group.create_group("data")
            for segment_id, segment_data in data.items():
                features = np.array(segment_data["features"], dtype=np.float32)
                intervals = np.array(segment_data["intervals"], dtype=np.float32)
                seg_group = data_group.create_group(segment_id)
                seg_group.create_dataset("features", data=features, compression='gzip')
                seg_group.create_dataset("intervals", data=intervals, compression='gzip')
            
            # åˆ›å»º metadata ç»„
            metadata_group = top_group.create_group("metadata")
            for key, value in metadata.items():
                try:
                    if key == "md5" and value is None:
                        value = ""  # æˆ–è€…ä½¿ç”¨ "None"
                    # å¦‚æœå€¼æ˜¯ None æˆ–å…¶ä»–å¤æ‚ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    elif value is None or isinstance(value, (list, tuple, dict)):
                        value = str(value)
                    metadata_group.attrs[key] = value
                except Exception as e:
                    print(f"âŒ Error setting metadata attribute {key}: {e}")
            
            # æ·»åŠ  description å±æ€§
            metadata_group.attrs["description"] = description
        
        print(f"âœ… CSD æ–‡ä»¶ä¿å­˜æˆåŠŸ: {output_path}")

    def process_labels_csv_to_csd(self, csv_path):
        """
        å°†æ‚¨çš„meta.csvè½¬æ¢ä¸ºMOSEIæ ¼å¼çš„labelsæ•°æ®
        
        Args:
            csv_path (str): meta.csvæ–‡ä»¶è·¯å¾„
            
        Returns:
            dict: labelsæ•°æ®å­—å…¸
        """
        print(f"Processing labels from {csv_path}")
        
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_path)
        
        labels_data = {}
        
        for _, row in df.iterrows():
            try:
                # æ ¹æ®æ‚¨çš„CSVæ ¼å¼ï¼š
                # ç¬¬1åˆ—ï¼švideo_id (æ–‡ä»¶å¤¹å)
                # ç¬¬2åˆ—ï¼šclip_id (æ–‡ä»¶å)  
                # ç¬¬4åˆ—ï¼šlabel (æ•´ä½“æƒ…æ„Ÿæ ‡ç­¾)
                # ç¬¬5åˆ—ï¼šlabel_T (æ–‡æœ¬æƒ…æ„Ÿæ ‡ç­¾)
                # ç¬¬6åˆ—ï¼šlabel_A (éŸ³é¢‘æƒ…æ„Ÿæ ‡ç­¾)
                # ç¬¬7åˆ—ï¼šlabel_V (è§†é¢‘æƒ…æ„Ÿæ ‡ç­¾)
                
                video_id = str(row.iloc[0])     # video_0001
                clip_id = str(row.iloc[1])      # 0001
                overall_label = float(row.iloc[2])  # label
                
                # ä½¿ç”¨video_id + clip_idä½œä¸ºsegment_idï¼Œä¸è§†é¢‘æ–‡ä»¶å¯¹åº”
                segment_id = f"{video_id}_{clip_id}"
                
                # åˆ›å»ºMOSEIæ ‡ç­¾æ ¼å¼ - ä½¿ç”¨æ•´ä½“æ ‡ç­¾
                labels_data[segment_id] = {
                    'features': np.array([[overall_label]], dtype=np.float32),
                    'intervals': np.array([[0, 1]], dtype=np.int32)
                }
                
            except Exception as e:
                print(f"Error processing label for row {_}: {e}")
                # ä½¿ç”¨é»˜è®¤ä¸­æ€§æ ‡ç­¾
                segment_id = f"error_{_}"
                labels_data[segment_id] = {
                    'features': np.array([[0.0]], dtype=np.float32),
                    'intervals': np.array([[0, 1]], dtype=np.int32)
                }
        
        print(f"âœ“ Converted {len(labels_data)} labels")
        return labels_data

    def process_video_to_accurate_mosei_format(self, video_path):
        """
        å°†MP4è§†é¢‘è½¬æ¢ä¸ºæ›´å‡†ç¡®çš„MOSEIæ•°æ®é›†æ ¼å¼
        
        Returns:
            dict: åŒ…å«ä¸‰ä¸ªæ¨¡æ€çš„å‡†ç¡®ç‰¹å¾å­—å…¸
        """
        print(f"Processing video with accurate MOSEI features: {video_path}")
        
        # 1. æå–è¯åµŒå…¥åºåˆ—
        print("Extracting word-based language features...")
        word_features = self.extract_word_features(video_path)
        
        # 2. æå–COVAREPéŸ³é¢‘ç‰¹å¾
        print("Extracting COVAREP acoustic features...")
        covarep_features = self.extract_covarep_acoustic_features(video_path)
        
        # 3. æå–OpenFaceè§†è§‰ç‰¹å¾
        print("Extracting OpenFace visual features...")
        openface_features = self.extract_openface_visual_features(video_path)
        
        # 4. ç»„ç»‡ä¸ºMOSEIæ ¼å¼
        word_intervals = np.array([[i, i + 1] for i in range(len(word_features))], dtype=np.float32) if word_features else np.array([[0, 1]], dtype=np.float32)
        covarep_intervals = np.array([[i, i + 1] for i in range(len(covarep_features))], dtype=np.float32) if len(covarep_features) > 0 else np.array([[0, 1]], dtype=np.float32)
        openface_intervals = np.array([[i, i + 1] for i in range(len(openface_features))], dtype=np.float32) if len(openface_features) > 0 else np.array([[0, 1]], dtype=np.float32)

        mosei_data = {
            "language": {
                "features": word_features,  # [[word_embedding, start, end], ...]
                "intervals": word_intervals
            },
            "acoustic": {
                "features": covarep_features,  # [æ—¶é—´æ­¥æ•°,40ç»´]
                "intervals": covarep_intervals
            },
            "visual": {
                "features": openface_features,  # [æ—¶é—´æ­¥æ•°, 709ç»´] 
                "intervals": openface_intervals
            }
        }
        
        print(f"âœ“ Language: {len(word_features)} words with embeddings")
        print(f"âœ“ Acoustic: {covarep_features.shape} COVAREP features")
        print(f"âœ“ Visual: {openface_features.shape} OpenFace features")
        
        return mosei_data

def process_video_dataset_to_accurate_mosei_from_csv(csv_path, video_base_dir, output_dir, language="zh"):
    """
    æ ¹æ®æ‚¨çš„Excel/CSVæ–‡ä»¶ç»“æ„å¤„ç†è§†é¢‘æ•°æ®é›†ï¼Œç”Ÿæˆmmdatasdkå…¼å®¹çš„.csdæ–‡ä»¶
    
    Args:
        csv_path (str): meta.csvæˆ–meta.xlsxæ–‡ä»¶è·¯å¾„
        video_base_dir (str): è§†é¢‘æ–‡ä»¶æ ¹ç›®å½•
        output_dir (str): è¾“å‡ºç›®å½•
        language (str): è¯­è¨€ä»£ç 
    """
    extractor = AccurateMOSEIExtractor(language=language)
    
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹å¼
    file_path = Path(csv_path)
    file_ext = file_path.suffix.lower()
    
    print(f"ğŸ“Š è¯»å–æ•°æ®æ–‡ä»¶: {csv_path}")
    
    try:
        if file_ext == '.csv':
            print("ğŸ“‹ æ£€æµ‹åˆ°CSVæ–‡ä»¶ï¼Œä½¿ç”¨pandas.read_csv()...")
            # CSVä¹ŸæŒ‡å®šå‰ä¸¤åˆ—ä¸ºå­—ç¬¦ä¸²
            df = pd.read_csv(csv_path, dtype={0: str, 1: str})
        else:
            print(f"âš ï¸ æœªçŸ¥æ–‡ä»¶æ ¼å¼ {file_ext}ï¼Œå°è¯•ä½œä¸ºCSVè¯»å–...")
            df = pd.read_csv(csv_path, dtype={0: str, 1: str})
            
        print(f"âœ… æ•°æ®æ–‡ä»¶è¯»å–æˆåŠŸ: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
        
        # æ˜¾ç¤ºå‰å‡ è¡Œçš„è·¯å¾„ä¿¡æ¯ç”¨äºéªŒè¯
        print("ğŸ” å‰3è¡Œè·¯å¾„ä¿¡æ¯éªŒè¯:")
        for i in range(min(3, len(df))):
            video_id = str(df.iloc[i, 0]).strip()
            clip_id = str(df.iloc[i, 1]).strip()
            print(f"  è¡Œ{i}: video_id='{video_id}', clip_id='{clip_id}' -> {video_id}/{clip_id}.mp4")
        
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿:")
        print("  1. æ–‡ä»¶è·¯å¾„æ­£ç¡®")
        print("  2. å¦‚æœæ˜¯Excelæ–‡ä»¶ï¼Œè¯·å®‰è£…: pip install openpyxl")
        print("  3. æ–‡ä»¶æ ¼å¼æ­£ç¡®ä¸”å¯è¯»")
        raise e
    
    video_base_dir = Path(video_base_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # åˆ›å»ºMOSEIå…¼å®¹æ•°æ®ç»“æ„
    dataset = {
        "language": {},
        "acoustic": {},
        "visual": {},
        "labels": {}
    }
    
    # å¤„ç†æ¯ä¸€è¡Œè§†é¢‘
    processed_count = 0
    error_count = 0
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f"Processing videos with {language} MOSEI features"):
        try:
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®ï¼Œå¹¶å»é™¤å¯èƒ½çš„ç©ºæ ¼
            video_id = str(row.iloc[0]).strip()
            clip_id = str(row.iloc[1]).strip()
            
            print(f"ğŸ” å¤„ç†è¡Œ{idx}: video_id='{video_id}', clip_id='{clip_id}'")
            
            # å¤„ç†æ ‡ç­¾æ•°æ®ï¼Œç¡®ä¿æ˜¯æ•°å€¼ç±»å‹
            try:
                overall_label = float(pd.to_numeric(row.iloc[2], errors='coerce'))
                text_label = float(pd.to_numeric(row.iloc[3], errors='coerce')) if len(row) > 3 else overall_label
                audio_label = float(pd.to_numeric(row.iloc[4], errors='coerce')) if len(row) > 4 else overall_label
                vis_label = float(pd.to_numeric(row.iloc[5], errors='coerce')) if len(row) > 5 else overall_label
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆå€¼
                if pd.isna(overall_label):
                    print(f"âš ï¸ è¡Œ {idx}: overall_label æ— æ•ˆï¼Œè·³è¿‡")
                    continue
                    
            except (ValueError, TypeError) as e:
                print(f"âš ï¸ è¡Œ {idx}: æ ‡ç­¾æ•°æ®è½¬æ¢å¤±è´¥ {e}ï¼Œè·³è¿‡")
                continue

            # æ„å»ºè§†é¢‘è·¯å¾„
            video_path = video_base_dir / video_id / f"{clip_id}.mp4"
            print(f"ğŸ“ æ„å»ºè§†é¢‘è·¯å¾„: {video_path}")
            
            if not video_path.exists():
                print(f"âš ï¸ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                
                # è¯¦ç»†æ£€æŸ¥è·¯å¾„é—®é¢˜
                parent_dir = video_path.parent
                if parent_dir.exists():
                    print(f"ğŸ“‚ çˆ¶ç›®å½•å­˜åœ¨: {parent_dir}")
                    mp4_files = list(parent_dir.glob("*.mp4"))
                    print(f"ğŸ“„ ç›®å½•ä¸­çš„MP4æ–‡ä»¶: {[f.name for f in mp4_files]}")
                    
                    # æŸ¥çœ‹æ˜¯å¦æœ‰ç›¸ä¼¼çš„æ–‡ä»¶å
                    for mp4_file in mp4_files:
                        if mp4_file.stem == clip_id or mp4_file.stem == clip_id.lstrip('0'):
                            print(f"ğŸ’¡ å¯èƒ½çš„åŒ¹é…æ–‡ä»¶: {mp4_file.name}")
                else:
                    print(f"ğŸ“‚ çˆ¶ç›®å½•ä¸å­˜åœ¨: {parent_dir}")
                
                error_count += 1
                continue

            print(f"ğŸ¬ å¤„ç†è§†é¢‘: {video_id}/{clip_id}")
            mosei_data = extractor.process_video_to_accurate_mosei_format(str(video_path))
            segment_id = f"{video_id}_{clip_id}"

            dataset["language"][segment_id] = mosei_data["language"]
            dataset["acoustic"][segment_id] = mosei_data["acoustic"]  
            dataset["visual"][segment_id] = mosei_data["visual"]

            # åˆå¹¶æ ‡ç­¾ä¸ºä¸€ä¸ªå¤šç»´æ ‡ç­¾
            num_labels = 4  # æ ‡ç­¾çš„ç»´åº¦æ•°é‡ï¼Œä¾‹å¦‚æ•´ä½“æƒ…æ„Ÿã€æ–‡æœ¬æƒ…æ„Ÿã€éŸ³é¢‘æƒ…æ„Ÿã€è§†è§‰æƒ…æ„Ÿ
            dataset["labels"][segment_id] = {
                'features': np.array([[overall_label, text_label, audio_label, vis_label]], dtype=np.float32),
                'intervals': np.array([[i, i + 1] for i in range(num_labels)], dtype=np.int32)  # åŠ¨æ€ç”Ÿæˆ intervals
            }

            processed_count += 1
            print(f"âœ“ æˆåŠŸå¤„ç†: {video_id}/{clip_id} | æ ‡ç­¾: O={overall_label:.2f}, T={text_label:.2f}, A={audio_label:.2f}, V={vis_label:.2f}")

        except Exception as e:
            error_count += 1
            print(f"âœ— å¤„ç†é”™è¯¯ {video_id}/{clip_id}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"  âœ… æˆåŠŸå¤„ç†: {processed_count} ä¸ªè§†é¢‘")
    print(f"  âŒ å¤„ç†å¤±è´¥: {error_count} ä¸ªè§†é¢‘")
    
    if processed_count == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•è§†é¢‘ï¼Œè¯·æ£€æŸ¥:")
        print("  1. è§†é¢‘æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("  2. æ•°æ®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
        print("  3. æ ‡ç­¾æ•°æ®æ˜¯å¦æœ‰æ•ˆ")
        return dataset
    
    # ä¿å­˜ä¸ºmmdatasdkå…¼å®¹çš„.csdæ ¼å¼æ–‡ä»¶
    print("ğŸ’¾ ä¿å­˜ä¸ºmmdatasdkå…¼å®¹çš„.csdæ ¼å¼...")
    metadata_acoustic = {
    "alignment compatible": True,
    "computational sequence description": "COVAREP Acoustic Features for CMU-MOSEI Dataset",
    "computational sequence version": "1.0",
    "contact": "abagherz@andrew.cmu.edu",
    "creator": "Amir Zadeh",
    "dataset bib citation": "@inproceedings{cmumoseiacl2018, title={Multimodal Language Analysis in the Wild: {CMU-MOSEI} Dataset and Interpretable Dynamic Fusion Graph}, author={Zadeh, Amir and Liang, Paul Pu and Vanbriesen, Jon and Poria, Soujanya and Cambria, Erik and Chen, Minghai and Morency, Louis-Philippe},booktitle={Association for Computational Linguistics (ACL)},year={2018}}",
    "dataset name": "CMU-MOSEI",
    "dataset version": "1.0",
    "dimension names": ['F0', 'VUV', 'NAQ', 'QOQ', 'H1H2', 'PSP', 'MDQ', 'peakSlope', 'Rd', 'Rd_conf', 'creak', 'MCEP_0', 'MCEP_1', 'MCEP_2', 'MCEP_3', 'MCEP_4', 'MCEP_5', 'MCEP_6', 'MCEP_7', 'MCEP_8', 'MCEP_9', 'MCEP_10', 'MCEP_11', 'MCEP_12', 'MCEP_13', 'MCEP_14', 'MCEP_15', 'MCEP_16', 'MCEP_17', 'MCEP_18', 'MCEP_19', 'MCEP_20', 'MCEP_21', 'MCEP_22', 'MCEP_23', 'MCEP_24', 'HMPDM_0', 'HMPDM_1', 'HMPDM_2', 'HMPDM_3', 'HMPDM_4', 'HMPDM_5', 'HMPDM_6', 'HMPDM_7', 'HMPDM_8', 'HMPDM_9', 'HMPDM_10', 'HMPDM_11', 'HMPDM_12', 'HMPDM_13', 'HMPDM_14', 'HMPDM_15', 'HMPDM_16', 'HMPDM_17', 'HMPDM_18', 'HMPDM_19', 'HMPDM_20', 'HMPDM_21', 'HMPDM_22', 'HMPDM_23', 'HMPDM_24', 'HMPDD_0', 'HMPDD_1', 'HMPDD_2', 'HMPDD_3', 'HMPDD_4', 'HMPDD_5', 'HMPDD_6', 'HMPDD_7', 'HMPDD_8', 'HMPDD_9', 'HMPDD_10', 'HMPDD_11', 'HMPDD_12'],
    "featureset bib citation": "@inproceedings{degottex2014covarep,title={COVAREP-A collaborative voice analysis repository for speech technologies},author={Degottex, Gilles and Kane, John and Drugman, Thomas and Raitio, Tuomo and Scherer, Stefan},booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},pages={960--964},year={2014},organization={IEEE}}",
    "md5": None,
    "root name": "COVAREP",
    "uuid": "af272a08-bb43-442d-b7d5-e7216a4c5119",
    }
    metadata_language = {
    "alignment compatible": True,
    "computational sequence description": "Word vector sequences for CMU-MOSEI Dataset",
    "computational sequence version": "1.0",
    "contact": "abagherz@andrew.cmu.edu",
    "creator": "Amir Zadeh",
    "dataset bib citation": "@inproceedings{cmumoseiacl2018, title={Multimodal Language Analysis in the Wild: {CMU-MOSEI} Dataset and Interpretable Dynamic Fusion Graph}, author={Zadeh, Amir and Liang, Paul Pu and Vanbriesen, Jon and Poria, Soujanya and Cambria, Erik and Chen, Minghai and Morency, Louis-Philippe},booktitle={Association for Computational Linguistics (ACL)},year={2018}}",
    "dataset name": "CMU-MOSEI",
    "dataset version": "1.0",
    "dimension names": ["vector"] * 300,  # å‡è®¾è¯å‘é‡ç»´åº¦ä¸º 300
    "featureset bib citation": "@article{P2FA,title={Speaker identification on the SCOTUS corpus},author={Yuan, Jiahong and Liberman, Mark},journal={Journal of the Acoustical Society of America},volume={123},number={5},pages={3878},year={2008},publisher={[New York: Acoustical Society of America]}}",
    "md5": None,
    "root name": "glove_vectors",
    "uuid": "8ac9704c-49b3-40ba-8c37-f029d3ddce43",
    }
    metadata_visual = {
    "alignment compatible": True,
    "computational sequence description": "FACET 4.2 Visual Features for CMU-MOSEI Dataset",
    "computational sequence version": "1.0",
    "contact": "abagherz@andrew.cmu.edu",
    "creator": "Amir Zadeh",
    "dataset bib citation": "@inproceedings{cmumoseiacl2018, title={Multimodal Language Analysis in the Wild: {CMU-MOSEI} Dataset and Interpretable Dynamic Fusion Graph}, author={Zadeh, Amir and Liang, Paul Pu and Vanbriesen, Jon and Poria, Soujanya and Cambria, Erik and Chen, Minghai and Morency, Louis-Philippe},booktitle={Association for Computational Linguistics (ACL)},year={2018}}",
    "dataset name": "CMU-MOSEI",
    "dataset version": "1.0",
    "dimension names": ['Anger', 'Contempt', 'Disgust', 'Joy', 'Fear', 'Baseline', 'Sadness', 'Surprise', 'Confusion', 'Frustration', 'AU1', 'AU2', 'AU4', 'AU5', 'AU6', 'AU7', 'AU9', 'AU10', 'AU12', 'AU14', 'AU15', 'AU17', 'AU18', 'AU20', 'AU23', 'AU24', 'AU25', 'AU26', 'AU28', 'AU43', 'Has_Glasses', 'Is_Male', 'Pitch', 'Yaw', 'Roll'],
    "featureset bib citation": "@online{emotient,author = {iMotions},title = {Facial Expression Analysis},year = {2017},url = {goo.gl/1rh1JN}}",
    "md5": None,
    "root name": "FACET 4.2",
    "uuid": "f592e140-2766-426b-add3-8a14498059e7",
    }
    metadata_labels = {
    "alignment compatible": True,
    "computational sequence description": "Labels for CMU-MOSEI Dataset",
    "computational sequence version": "1.0",
    "contact": "abagherz@andrew.cmu.edu",
    "creator": "Amir Zadeh",
    "dataset bib citation": "@inproceedings{cmumoseiacl2018, title={Multimodal Language Analysis in the Wild: {CMU-MOSEI} Dataset and Interpretable Dynamic Fusion Graph}, author={Zadeh, Amir and Liang, Paul Pu and Vanbriesen, Jon and Poria, Soujanya and Cambria, Erik and Chen, Minghai and Morency, Louis-Philippe},booktitle={Association for Computational Linguistics (ACL)},year={2018}}",
    "dataset name": "CMU-MOSEI",
    "dataset version": "1.0",
    "dimension names": ['sentiment', 'happy', 'sad', 'anger', 'surprise', 'disgust', 'fear'],
    "featureset bib citation": "@online{amt, author = {Amazon},title = {Amazon Mechanical Turk},year = {2017},url = {https://www.mturk.com}}",
    "md5": None,
    "root name": "All Labels",
    "uuid": "bbce9ca9-e556-46f4-823e-7c5e0147afab",
    }

    extractor.save_to_csd_format(
        dataset["language"], 
        output_dir / "CMU_MOSEI_TimestampedWordVectors.csd",
        description="language",
        metadata=metadata_language
    )

    extractor.save_to_csd_format(
        dataset["acoustic"], 
        output_dir / "CMU_MOSEI_COVAREP.csd",
        description="acoustic",
        metadata=metadata_acoustic
    )

    extractor.save_to_csd_format(
        dataset["visual"], 
        output_dir / "CMU_MOSEI_VisualFacet42.csd",
        description="visual",
        metadata=metadata_visual
    )

    extractor.save_to_csd_format(
        dataset["labels"], 
        output_dir / "CMU_MOSEI_Labels.csd",
        description="labels",
        metadata=metadata_labels
    )
    
    language_model = "Chinese FastText/Word2Vec" if language in ["zh", "chinese", "ä¸­æ–‡"] else "English GloVe"
    
    print(f"\nâœ… å¤„ç†å®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - CMU_MOSEI_TimestampedWordVectors.csd ({language_model})")
    print(f"  - CMU_MOSEI_COVAREP.csd") 
    print(f"  - CMU_MOSEI_VisualFacet42.csd")
    print(f"  - CMU_MOSEI_Labels.csd")
    
    # æµ‹è¯•å…¼å®¹æ€§
    print("\nğŸ§ª æµ‹è¯•mmdatasdkå…¼å®¹æ€§...")
    try:
        from mmsdk import mmdatasdk as md
        
        dataset_paths = {
            "language": str(output_dir / "CMU_MOSEI_TimestampedWordVectors.csd"),
            "acoustic": str(output_dir / "CMU_MOSEI_COVAREP.csd"),
            "visual": str(output_dir / "CMU_MOSEI_VisualFacet42.csd"),
            "labels": str(output_dir / "CMU_MOSEI_Labels.csd")
        }

        mosei_dataset = md.mmdataset(dataset_paths)
        
        print("âœ… mmdatasdkåŠ è½½æˆåŠŸ!")
        
        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        for modality in ["language", "acoustic", "visual", "labels"]:
            if modality in mosei_dataset:
                data = mosei_dataset[modality]
                print(f"  ğŸ“Š {modality}: {len(data)} segments")
                
                if len(data) > 0:
                    first_key = list(data.keys())[0]
                    features = data[first_key]["features"]
                    intervals = data[first_key]["intervals"]
                    print(f"    æ ·æœ¬ '{first_key}': features {features.shape}, intervals {intervals.shape}")
        
        print("ğŸ‰ å¯ä»¥åœ¨åŸMOSEIé¡¹ç›®ä¸­ä½¿ç”¨!")
        
    except Exception as e:
        print(f"âŒ mmdatasdkå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·æ‰‹åŠ¨æ£€æŸ¥ç”Ÿæˆçš„.csdæ–‡ä»¶")
    
    return dataset

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ ¹æ®æ‚¨çš„æ•°æ®ç»“æ„å¤„ç†
    dataset = process_video_dataset_to_accurate_mosei_from_csv(
        csv_path="our_MSA/meta_test_only.csv",
        video_base_dir="our_MSA/ch_video",
        output_dir="our_MSA/ch_video_preprocess",
        language="zh"
    )