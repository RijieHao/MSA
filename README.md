# ğŸŒ Cross-Lingual Multimodal Sentiment Analysis (MSA)

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)]()
[![PyTorch](https://img.shields.io/badge/PyTorch-2.2+-ee4c2c.svg)]()
[![License](https://img.shields.io/badge/license-MIT-green.svg)]()

> **Chineseâ€“English Cross-Lingual Multimodal Sentiment Analysis Framework**
> ğŸ¥ End-to-end **Video â†’ Sentiment** prediction
> ğŸ”— Transformer-based **Cross-Attention Fusion**
> ğŸ” Built-in **Explainability & Visualization**
> ğŸ† Developed for **MSA Challenge @ The 4th Pazhou AI Competition**

---

## âœ¨ Key Features

- ğŸŒ **Cross-Lingual**: Handles both **Chinese** and **English** with **Whisper + BERT**.
- ğŸ¬ **Multimodal**:
  - **Text** â†’ Whisper transcript + BERT embeddings
  - **Audio** â†’ MFCC + prosody features
  - **Visual** â†’ MediaPipe landmarks + HoG + FACS
- ğŸ”— **Fusion**: Transformer-based **Cross-Attention** for balanced modality contribution.
- ğŸ” **Explainability**: Attention heatmaps reveal modality importance.
- âš¡ **Flexible Execution**:
  - One-click demo (test_script.py)
  - Two-step reproducible pipeline (video â†’ features â†’ training)
  - Direct video-to-prediction (no `.pkl` saved)
- ğŸ§© **Modular Design**: Easily extend with CLIP, SAM, LLMs, etc.

---

## ğŸ“‚ Project Structure

```bash
MSA/
â”œâ”€â”€ README.md                # Project introduction
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ test_script.py           # One-click testing
â”œâ”€â”€ best_models/             # Pre-trained weights (en.pt / zh.pt)
â”œâ”€â”€ MSAbypkl/                # PKL-based workflow (train/eval)
â”‚   â”œâ”€â”€ main.py		     # Training & evaluation
|   â”œâ”€â”€ config.py            # Modify parameters
â”‚   â”œâ”€â”€ scripts/             # Training scripts
â”‚   â””â”€â”€ src/                 # Core modules (data/models/training/utils)
â”œâ”€â”€ MSAbyvideo/              # Direct video-to-sentiment workflow
â”œâ”€â”€ video2pkl/               # Video-to-feature extractor
â”‚   â””â”€â”€ video2pkl.py         # Whisper + BERT + MediaPipe extractor
â””â”€â”€ Test_Data / Test_Results # Example data & outputs
```

ğŸ“– More details: 

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ One-Click Execution (Recommended)

```bash
python test_script.py
```

- Put videos into `Video_Data/`
- Predictions saved in `Test_Results/`

### 3ï¸âƒ£ Two-Step Workflow

**Step 1: Extract Features**

```bash
python video2pkl/video2pkl/video2pkl.py     --language en     --video_dir ./Video_Data     --csv_path ./meta.csv     --output_dir ./MSAbypkl/data/data_pkl/myset
```

**Step 2: Train / Evaluate**

```python
# config.py
DATASET_NAME = "myset"
```

```bash
python MSAbypkl/main.py
```

### 4ï¸âƒ£ Direct Video-to-Sentiment

```bash
python MSAbyvideo/main.py
```

- Interactive CLI prompts
- Faster, no `.pkl` saved

---

## ğŸ§  Model Design

- **Text**: Whisper â†’ BERT (768-dim)
- **Audio**: MFCC + prosody (40-dim)
- **Visual**: MediaPipe + HoG + FACS (35-dim)
- **Fusion**: Transformer encoders + Cross-Attention
- **Output**: 5-class sentiment â†’ `SNEG | WNEG | NEUT | WPOS | SPOS`
  *(Regression supported via `NUM_CLASSES=1`)*

ğŸ“Š **Metrics**: Accuracy, Macro-F1, Confusion Matrix

---

## ğŸ“Š Results

- ğŸ† **Public Leaderboard**: `0.4350` (baseline-level, CPU-only training)
- ğŸ”¬ **Ablations**: Cross-attention > Early fusion > Late fusion
- âš¡ With GPU, accuracy expected to improve significantly

---

## ğŸ” Explainability

- Saves **cross-attention weights**
- Generates **modality contribution heatmaps**
- Ensures **transparent & trustworthy predictions**

---

## ğŸ“¬ Contact

ğŸ‘¨â€ğŸ’» **Team Members**: YanSong Hu Â· YangLe Ma Â· ZhouYang Wang Â· RiJie Hao
ğŸ“§ Email: 672416680@qq.com | mylsxxy@163.com

â­ If you like this project, please **star the repo** â€” it keeps us motivated! â­
